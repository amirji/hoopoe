{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd600bed",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f09bd2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pdftotext\n",
    "from PyPDF2 import PdfFileReader\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from py2neo import Graph\n",
    "from py2neo.bulk import create_nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed48285",
   "metadata": {},
   "source": [
    "#### Initialize Graph Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2122210a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>n</th><th>n_sq</th></tr><tr><td style=\"text-align:right\">1</td><td style=\"text-align:right\">1</td></tr><tr><td style=\"text-align:right\">2</td><td style=\"text-align:right\">4</td></tr><tr><td style=\"text-align:right\">3</td><td style=\"text-align:right\">9</td></tr></table>"
      ],
      "text/plain": [
       " n | n_sq \n",
       "---|------\n",
       " 1 |    1 \n",
       " 2 |    4 \n",
       " 3 |    9 "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = Graph(\"http://localhost:7474/\", auth=(\"neo4j\", \"berjis89\"))\n",
    "graph.run(\"UNWIND range(1, 3) AS n RETURN n, n * n as n_sq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b17ff539",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDict(dict):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "    def __getitem__(self,key):\n",
    "        return dict.__getitem__(self, key.lower())\n",
    "    def __setitem__(self,key,value):\n",
    "        return dict.__setitem__(self,key.lower(), value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "1cde8ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(folder_name):\n",
    "    \"\"\"\n",
    "    This function assumes that the folder \"folder_name\" is stored inside the the notebooks folder.\n",
    "    Params:\n",
    "    folder_name: string\n",
    "    Returns: the paths to the data files in the given folder as a list.\n",
    "    \"\"\"\n",
    "    arr = os.listdir(folder_name)\n",
    "    return [os.path.join(os.getcwd(),folder_name, a) for a in arr]\n",
    "\n",
    "\n",
    "def pdftotext_wrapper(input_file, options=None, output_file=None):\n",
    "    \"\"\"\n",
    "    This function wraps the pdftotext command line tool.\n",
    "    Params:\n",
    "    input_file: string of path to the input pdf file.\n",
    "    output_file: string of path to the output text file.\n",
    "    options: string\n",
    "    Returns: the text as a string.\n",
    "    \"\"\"\n",
    "    if options is None:\n",
    "        options = \"\"\n",
    "\n",
    "    if output_file is None:\n",
    "        output_file = \"\"\n",
    "\n",
    "    check = os.popen(\"pdftotext \" + options + \" \" + input_file + \" \" + output_file).read()\n",
    "    if check == \"\":\n",
    "        return \"Success\"\n",
    "\n",
    "\n",
    "def extract_text(path, method):\n",
    "    \"\"\"\n",
    "    This function extracts the text from a pdf file.\n",
    "    Params:\n",
    "    path: string\n",
    "    method: string\n",
    "    Returns: the text as a string.\n",
    "    \"\"\"\n",
    "    if method == \"pdftotext_cli\":\n",
    "        file_name = path.replace(os.path.dirname(data[0])+\"/\", \"\").replace(\".pdf\", \"\")\n",
    "        output_dir = os.path.join(os.getcwd(), \"texts\")\n",
    "        output_file = os.path.join(output_dir, file_name + \".txt\")\n",
    "        pdftotext_wrapper(data[0], \"-raw\", output_file) \n",
    "        with open(output_file, 'r') as f:\n",
    "            #return f.read()\n",
    "            return f.read().replace(\"\\n\", \" \")\n",
    "            #return f.readlines()\n",
    "\n",
    "    if method == \"pdftotext_python\":\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pdftotext.PDF(f)\n",
    "\n",
    "    if method == \"pypdf2\":\n",
    "        text = []\n",
    "        with open(path, \"rb\") as f:\n",
    "            pdf = PdfFileReader(f)\n",
    "            text = [pdf.getPage(i).extractText() for i in range(pdf.numPages)]\n",
    "            return text\n",
    "\n",
    "def extract_entities(quote):\n",
    "    words = word_tokenize(quote)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    tree = nltk.ne_chunk(tags, binary=False)\n",
    "    return set(\n",
    "        \" \".join(i[0] for i in t)\n",
    "        for t in tree if hasattr(t, \"label\") and t.label() != \"NE\"\n",
    "    )\n",
    "\n",
    "def extract_info(folder, source=\"Tex\"):\n",
    "    \"\"\"\n",
    "    This function extracts the information from a paper using different methods and returns it as a dictionary with the following keys:\n",
    "    {\n",
    "        \"Author/Authors\": string,\n",
    "        \"Title\": string,\n",
    "        \"Year\": string,\n",
    "        \"Journal\": string,\n",
    "        \"Volume\": string,\n",
    "        \"Pages\": string,\n",
    "        \"Abstract\": string,\n",
    "        \"Sections\": list of strings,\n",
    "        \"References_Sections\": list of pairs of strings (refrence, section),\n",
    "        \"refrences\": list of strings,\n",
    "        \"Keywords\": string,\n",
    "        \"Language\": string,\n",
    "        \"Source\": string,\n",
    "        }\n",
    "    Params:\n",
    "    paper: string\n",
    "    source: string\n",
    "    Returns: a dictionary with the extracted information.\n",
    "\n",
    "    Reg Tips:\n",
    "    1. r\"\\\\author.*?\\\\\\\\\" -> mathc from \\authors command till the first \\\\ using lazy match in the Tex file.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    info = MyDict()\n",
    "\n",
    "    if source != \"Tex\":\n",
    "        return info\n",
    "    \n",
    "\n",
    "    contents = get_data(folder)\n",
    " \n",
    "    tex_files = [ct for ct in contents if os.path.splitext(ct)[1] == '.tex']\n",
    "    bib_files = [ct for ct in contents if os.path.splitext(ct)[1] == '.bib']\n",
    "\n",
    "\n",
    "    text = \"\"\n",
    "    for tex_file in tex_files:\n",
    "        with open(tex_file, \"r\") as f:\n",
    "            temp = f.read()\n",
    "            text+= \"\\n\"+temp\n",
    "    \n",
    "    refs = \"\"\n",
    "    ref_style=\"\"\n",
    "    if len(bib_files) != 0:\n",
    "        ref_style=\"file\"\n",
    "        for bib_file in bib_files:\n",
    "            with open(bib_file, \"r\") as f:\n",
    "                temp = f.read()\n",
    "                refs+= \"\\n\"+temp\n",
    "        # Adding an \"@\" sign at the end of the refs text, it will help in the extracting data using regex:\n",
    "        refs+=\"\\n @\"\n",
    "    else:\n",
    "        ref_style=\"bibitem\"\n",
    "        refs = re.findall(r\"\\\\bibitem.*?\\\\end\", text, re.DOTALL)[0]\n",
    "        text = text.replace(refs,\"\")\n",
    "        refs+=\"\\n \\\\bibitem\"\n",
    "\n",
    "    # Remove all comments:\n",
    "    #text = re.sub(r\"\\%.*?\\n\", \"\", text,  re.DOTALL)\n",
    "\n",
    "    #tmp = re.findall(r\"\\@\\w+.[\\s]*?.?(?=\"+\"2017AREPS..45..359J\"+r\")(.*?)\\@\", refs, re.DOTALL)\n",
    "    #print(tmp)\n",
    "    \n",
    "    # Extracting the author(s)\n",
    "    #authors = re.findall(r\"\\\\author\\{(.*?)\\}\", text)\n",
    " \n",
    "    Author_main = re.findall(r\"\\\\author\\[(.*?)\\]\", text)\n",
    "    temp_authors = re.findall(r\"\\\\author.*?\\\\\\\\\", text, re.DOTALL)\n",
    "    temp_authors = re.findall(r\"\\].*?\\\\\\\\\", temp_authors[0], re.DOTALL)\n",
    "    temp_authors = re.findall(r\"(([A-Zéúßäüö]\\.?\\s?)*([A-Zéúßäüö][a-zéúßäüö]+\\.?\\s?)+([A-Zéúßäüö]\\.?\\s?[a-zéúßäüö]*)*)\", temp_authors[0], re.DOTALL)\n",
    "    Authors = [aut[0] for aut in temp_authors]\n",
    "\n",
    "\n",
    "    info[\"Author_main\"] = \"; \".join(Author_main)\n",
    "    info[\"Authors\"] = \"; \".join(Authors)\n",
    "\n",
    "    # Extracting the title\n",
    "    title_temp = re.findall(r\"\\\\title.*?]\", text, re.DOTALL)\n",
    "    title = re.findall(r\"\\[(.*?)]\", title_temp[0], re.DOTALL)\n",
    "    info[\"Title\"] = title[0]\n",
    "\n",
    "    # Extracting the Abstract\n",
    "    abstract_temp = re.findall(r\"\\\\begin{abstract}(.*?)\\\\end{abstract}\", text, re.DOTALL)\n",
    "    \n",
    "    # remove comments\n",
    "    abstract = re.sub(r\"\\%.*?\\n\", \"\", abstract_temp[0],  re.DOTALL)\n",
    "    info[\"Abstract\"] = abstract\n",
    "\n",
    "\n",
    "    # Find titles of sections\n",
    "    section_titles = re.findall(r\"\\\\section{(.*?)}\", text, re.DOTALL)\n",
    "\n",
    "    \n",
    "\n",
    "    # Extract text of each section:\n",
    "\n",
    "    sections_text = []\n",
    "    Sections = MyDict()\n",
    "    for s_t in section_titles:\n",
    "        section_grammer= r\"\\\\section{\" + s_t + \"}\" + r\"(.*?)\" + r\"\\\\section\"\n",
    "        temp =  re.findall(section_grammer, text, re.DOTALL)\n",
    "        sections_text.append(temp)\n",
    "        Sections[s_t] = MyDict({\"text\": temp[0]})\n",
    "        # Same data saved as flatten version to use in neo4j, later we need to disscuss\n",
    "        info[s_t + \" --text\"]=temp[0]\n",
    "        \n",
    "    info[\"Sections\"] = Sections\n",
    "\n",
    "    # Extracting the citeations in each section:\n",
    "   \n",
    "    if ref_style==\"file\":\n",
    "        info, citations_info = extract_file_ref_style(info, refs)\n",
    "    if ref_style==\"bibitem\":\n",
    "        info, citations_info = extract_bibitem_ref_style(info, refs)\n",
    "    \n",
    "    return info, citations_info\n",
    "\n",
    "def extract_bibitem_ref_style(info, refs):\n",
    "    cts = []\n",
    "    for s_t in info[\"Sections\"]:\n",
    "        cts_mtch = []\n",
    "        \n",
    "        # Citation style: \\cite[][]{ref1, ref2, ref3}     \n",
    "        temp_citep = re.findall(r\"\\\\citep.*?(\\[.*?\\])?{(.*?)}\", info[\"Sections\"][s_t][\"text\"], re.DOTALL)\n",
    "        if len(temp_citep)!=0:\n",
    "            cts_mtch.extend(temp_citep) \n",
    "        \n",
    "        # Citation style: \\citet[][]{ref1, ref2, ref3} \n",
    "        temp_citet = re.findall(r\"\\\\citet.*?(\\[.*?\\])?{(.*?)}\", info[\"Sections\"][s_t][\"text\"], re.DOTALL)\n",
    "        if len(temp_citet)!=0:\n",
    "            cts_mtch.extend(temp_citet)\n",
    "        \n",
    "        # Citation style: \\citealp[][]{ref1, ref2, ref3} \n",
    "        temp_citealp = re.findall(r\"\\\\citealp.*?(\\[.*?\\])?{(.*?)}\", info[\"Sections\"][s_t][\"text\"], re.DOTALL)\n",
    "        if len(temp_citealp)!=0:\n",
    "            cts_mtch.extend(temp_citealp)\n",
    "\n",
    "        cts_in_text = []\n",
    "        for c in cts_mtch:\n",
    "            if len(c)>=1:\n",
    "                if type(c[-1])==tuple:\n",
    "                    cts_in_text.append(list(c[-1]))\n",
    "                else:\n",
    "                    cts_in_text.append(c[-1])\n",
    "        \n",
    "         \n",
    "        info[\"Sections\"][s_t][\"citations\"] = [t.split(\",\") for t in cts_in_text]\n",
    "    \n",
    "        # Same data saved as flatten version to use in neo4j, later we need to disscuss\n",
    "        if len(cts_in_text)!=0:\n",
    "            cts = [t.split(\",\") for t in cts_in_text]\n",
    "\n",
    "        cts_flatten_temp= [c for ct in cts for c in ct]\n",
    "        cts_flatten = []\n",
    "        for c in cts_flatten_temp:\n",
    "            if type(c) is list:\n",
    "                cts_flatten.append(c[0].strip())\n",
    "            else:\n",
    "                cts_flatten.append(c.strip())\n",
    "\n",
    "        cts_info = []\n",
    "        for bib_item in cts_flatten:\n",
    "            bib_item_section = re.findall(r\"\\{[\\s]*?\"+bib_item+r\"[\\s]*?\\}.*?\\\\bibitem\", refs, re.DOTALL)\n",
    "            title = re.findall(r\"(?i)title.*?{(.*?)}\", bib_item_section[0],re.DOTALL)\n",
    "            doi = re.findall(r\"(?i)doi.*?{(.*?)}\", bib_item_section[0],re.DOTALL)\n",
    "            \n",
    "            cts_info.append([bib_item,title, doi])\n",
    "\n",
    "        info[s_t + \" --cts\"] = cts_info\n",
    "    \n",
    "    \n",
    "    return info, refs\n",
    "\n",
    "\n",
    "def extract_file_ref_style(info, refs):\n",
    "    cts = []\n",
    "    for s_t in info[\"Sections\"]:\n",
    "        cts_mtch = []\n",
    "        \n",
    "        # Citation style: \\cite[][]{ref1, ref2, ref3}     \n",
    "        temp_citep = re.findall(r\"\\\\citep.*?(\\[.*?\\])?{(.*?)}\", info[\"Sections\"][s_t][\"text\"], re.DOTALL)\n",
    "        if len(temp_citep)!=0:\n",
    "            cts_mtch.extend(temp_citep) \n",
    "        \n",
    "        # Citation style: \\citet[][]{ref1, ref2, ref3} \n",
    "        temp_citet = re.findall(r\"\\\\citet.*?(\\[.*?\\])?{(.*?)}\", info[\"Sections\"][s_t][\"text\"], re.DOTALL)\n",
    "        if len(temp_citet)!=0:\n",
    "            cts_mtch.extend(temp_citet)\n",
    "        \n",
    "        # Citation style: \\citealp[][]{ref1, ref2, ref3} \n",
    "        temp_citealp = re.findall(r\"\\\\citealp.*?(\\[.*?\\])?{(.*?)}\", info[\"Sections\"][s_t][\"text\"], re.DOTALL)\n",
    "        if len(temp_citealp)!=0:\n",
    "            cts_mtch.extend(temp_citealp)\n",
    "\n",
    "        cts_in_text = []\n",
    "        for c in cts_mtch:\n",
    "            if len(c)>=1:\n",
    "                if type(c[-1])==tuple:\n",
    "                    cts_in_text.append(list(c[-1]))\n",
    "                else:\n",
    "                    cts_in_text.append(c[-1])\n",
    "        \n",
    "        info[\"Sections\"][s_t][\"citations\"] = [t.split(\",\") for t in cts_in_text]\n",
    "    \n",
    "                # Same data saved as flatten version to use in neo4j, later we need to disscuss\n",
    "        if len(cts_in_text)!=0:\n",
    "            cts = [t.split(\",\") for t in cts_in_text]\n",
    "\n",
    "        cts_flatten_temp= [c for ct in cts for c in ct]\n",
    "        cts_flatten = []\n",
    "        for c in cts_flatten_temp:\n",
    "            if type(c) is list:\n",
    "                cts_flatten.append(c[0].strip())\n",
    "            else:\n",
    "                cts_flatten.append(c.strip())\n",
    "\n",
    "        cts_info = []\n",
    "        for bib_item in cts_flatten:\n",
    "            bib_item_section = re.findall(r\"\\@\\w+.[\\s]*?.?(?=[\\s]*?\"+c+r\")(.*?)\\@\", refs, re.DOTALL)\n",
    "            title = re.findall(r\"(?i)title.*?{(.*?)}\", bib_item_section[0],re.DOTALL)\n",
    "            doi = re.findall(r\"(?i)doi.*?{(.*?)}\", bib_item_section[0],re.DOTALL)\n",
    "            cts_info.append([bib_item,title, doi])\n",
    "\n",
    "        info[s_t + \" --cts\"] = cts_info\n",
    "\n",
    "    citations_info = MyDict()\n",
    "    # Adding citations from bib files to the sections:\n",
    "    bib_cts = []\n",
    "    all_bib_cts = []\n",
    "    for s_t in info[\"Sections\"]:\n",
    "        all_cts = info[\"Sections\"][s_t][\"citations\"]\n",
    "        for cts in all_cts:\n",
    "            for ct in cts:\n",
    "                if len(ct)==1:\n",
    "                    tmp = re.findall(r\"(\\@\\w+.[\\s]*?.?(?=[\\s]*?\"+ct[0]+r\")(.*?))\\@\", refs, re.DOTALL)\n",
    "                else:\n",
    "                    tmp = re.findall(r\"(\\@\\w+.[\\s]*?.?(?=[\\s]*?\"+ct+r\")(.*?))\\@\", refs, re.DOTALL)\n",
    "              #  print(tmp)\n",
    "                bib_cts.append(tmp)\n",
    "            all_bib_cts.append(bib_cts)\n",
    "        citations_info[s_t] = all_bib_cts\n",
    "    \n",
    "    return info, citations_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "99bf619c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/amir/Projects/papyrus/hoopoe/Texs/1511_03498'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = get_data(\"Texs\")\n",
    "papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "2ec02afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a0,b0 = extract_info(folder=papers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "efe10322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Espaillat14', [], ['10.2458/azu_uapress_9780816531240-ch022']],\n",
       " ['Andrews11', [], ['10.1088/2041-8205/742/1/L5']],\n",
       " ['Pietu05', [], ['10.1051/0004-6361:20042050']],\n",
       " ['Hughes07', [], ['10.1086/518885']],\n",
       " ['Casassus13', [], ['10.1038/nature11769']],\n",
       " ['Avenhaus14', [], ['10.1088/0004-637X/781/2/87']],\n",
       " ['Espaillat14', [], ['10.2458/azu_uapress_9780816531240-ch022']],\n",
       " ['Carmona14', [], ['10.1051/0004-6361/201322534']],\n",
       " ['Bruderer14', [], ['10.1051/0004-6361/201322857']],\n",
       " ['vanderMarel2015', [], ['10.1051/0004-6361/201525658']],\n",
       " ['Birnstiel2012', [], ['10.1051/0004-6361/201219262']],\n",
       " ['AlexanderPP6', [], ['10.2458/azu_uapress_9780816531240-ch021']],\n",
       " ['Owen11', [], ['10.1111/j.1365-2966.2010.17818.x']],\n",
       " ['Rosenfeld14', [], ['10.1088/0004-637X/782/2/62']],\n",
       " ['Rosenfeld14', [], ['10.1088/0004-637X/782/2/62']],\n",
       " ['crida06', [], ['10.1016/j.icarus.2005.10.007']],\n",
       " ['pm04', [], ['10.1051/0004-6361:200400053']],\n",
       " ['Fouchet07', [], ['10.1051/0004-6361:20077586']],\n",
       " ['Zhu12', [], ['10.1088/0004-637X/755/1/6']],\n",
       " ['Pinilla12', [], ['10.1051/0004-6361/201118204']],\n",
       " ['JohansenPP6', [], ['10.2458/azu_uapress_9780816531240-ch024']],\n",
       " ['Zhu12', [], ['10.1088/0004-637X/755/1/6']],\n",
       " ['Zhu14', [], ['10.1088/0004-637X/785/2/122']],\n",
       " ['Zhu11', [], ['10.1088/0004-637X/729/1/47']],\n",
       " ['Casassus13', [], ['10.1038/nature11769']],\n",
       " ['Fukagawa14', [], ['10.1093/pasj/65.6.L14']],\n",
       " ['vanderMarel13', [], ['10.1126/science.1236770']],\n",
       " ['Tang12', [], ['10.1051/0004-6361/201219414']],\n",
       " ['Andrews11all', [], ['10.1088/0004-637X/732/1/42']],\n",
       " ['Perez14', [], ['10.1088/2041-8205/783/1/L13']],\n",
       " ['Isella13', [], ['10.1088/0004-637X/775/1/30']],\n",
       " ['Brown09', [], ['10.1088/0004-637X/704/1/496']],\n",
       " ['Perez14', [], ['10.1088/2041-8205/783/1/L13']],\n",
       " ['vanDishoeck2015', [], []],\n",
       " ['lovelace99', [], ['10.1086/306900']],\n",
       " ['PP84', [], []],\n",
       " ['Bae15', [], ['10.1088/0004-637X/805/1/15']],\n",
       " ['Lyra09', [], ['10.1051/0004-6361:200810797']],\n",
       " ['LinMK12', [], ['10.1111/j.1365-2966.2012.21955.x']],\n",
       " ['VT06', [], ['10.1051/0004-6361:200500226']],\n",
       " ['Regaly12', [], ['10.1111/j.1365-2966.2011.19834.x']],\n",
       " ['Faure14', [], ['10.1051/0004-6361/201321911']],\n",
       " ['Lyra15', [], ['10.1051/0004-6361/201424919']],\n",
       " ['Flock15', [], ['10.1051/0004-6361/201424693']],\n",
       " ['ZhuStone14', [], ['10.1088/0004-637X/795/1/53']],\n",
       " ['Lesur14', [], ['10.1051/0004-6361/201423660']],\n",
       " ['Bai15', [], ['10.1088/0004-637X/798/2/84']],\n",
       " ['Gressel15', [], ['10.1088/0004-637X/801/2/84']]]"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a0[\"introduction --cts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "2ada40ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Espaillat14', [], ['10.2458/azu_uapress_9780816531240-ch022']],\n",
       " ['Andrews11', [], ['10.1088/2041-8205/742/1/L5']],\n",
       " ['Pietu05', [], ['10.1051/0004-6361:20042050']],\n",
       " ['Hughes07', [], ['10.1086/518885']],\n",
       " ['Casassus13', [], ['10.1038/nature11769']],\n",
       " ['Avenhaus14', [], ['10.1088/0004-637X/781/2/87']],\n",
       " ['Espaillat14', [], ['10.2458/azu_uapress_9780816531240-ch022']],\n",
       " ['Carmona14', [], ['10.1051/0004-6361/201322534']],\n",
       " ['Bruderer14', [], ['10.1051/0004-6361/201322857']],\n",
       " ['vanderMarel2015', [], ['10.1051/0004-6361/201525658']],\n",
       " ['Birnstiel2012', [], ['10.1051/0004-6361/201219262']],\n",
       " ['AlexanderPP6', [], ['10.2458/azu_uapress_9780816531240-ch021']],\n",
       " ['Owen11', [], ['10.1111/j.1365-2966.2010.17818.x']],\n",
       " ['Rosenfeld14', [], ['10.1088/0004-637X/782/2/62']],\n",
       " ['Rosenfeld14', [], ['10.1088/0004-637X/782/2/62']],\n",
       " ['crida06', [], ['10.1016/j.icarus.2005.10.007']],\n",
       " ['pm04', [], ['10.1051/0004-6361:200400053']],\n",
       " ['Fouchet07', [], ['10.1051/0004-6361:20077586']],\n",
       " ['Zhu12', [], ['10.1088/0004-637X/755/1/6']],\n",
       " ['Pinilla12', [], ['10.1051/0004-6361/201118204']],\n",
       " ['JohansenPP6', [], ['10.2458/azu_uapress_9780816531240-ch024']],\n",
       " ['Zhu12', [], ['10.1088/0004-637X/755/1/6']],\n",
       " ['Zhu14', [], ['10.1088/0004-637X/785/2/122']],\n",
       " ['Zhu11', [], ['10.1088/0004-637X/729/1/47']],\n",
       " ['Casassus13', [], ['10.1038/nature11769']],\n",
       " ['Fukagawa14', [], ['10.1093/pasj/65.6.L14']],\n",
       " ['vanderMarel13', [], ['10.1126/science.1236770']],\n",
       " ['Tang12', [], ['10.1051/0004-6361/201219414']],\n",
       " ['Andrews11all', [], ['10.1088/0004-637X/732/1/42']],\n",
       " ['Perez14', [], ['10.1088/2041-8205/783/1/L13']],\n",
       " ['Isella13', [], ['10.1088/0004-637X/775/1/30']],\n",
       " ['Brown09', [], ['10.1088/0004-637X/704/1/496']],\n",
       " ['Perez14', [], ['10.1088/2041-8205/783/1/L13']],\n",
       " ['vanDishoeck2015', [], []],\n",
       " ['lovelace99', [], ['10.1086/306900']],\n",
       " ['PP84', [], []],\n",
       " ['Bae15', [], ['10.1088/0004-637X/805/1/15']],\n",
       " ['Lyra09', [], ['10.1051/0004-6361:200810797']],\n",
       " ['LinMK12', [], ['10.1111/j.1365-2966.2012.21955.x']],\n",
       " ['VT06', [], ['10.1051/0004-6361:200500226']],\n",
       " ['Regaly12', [], ['10.1111/j.1365-2966.2011.19834.x']],\n",
       " ['Faure14', [], ['10.1051/0004-6361/201321911']],\n",
       " ['Lyra15', [], ['10.1051/0004-6361/201424919']],\n",
       " ['Flock15', [], ['10.1051/0004-6361/201424693']],\n",
       " ['ZhuStone14', [], ['10.1088/0004-637X/795/1/53']],\n",
       " ['Lesur14', [], ['10.1051/0004-6361/201423660']],\n",
       " ['Bai15', [], ['10.1088/0004-637X/798/2/84']],\n",
       " ['Gressel15', [], ['10.1088/0004-637X/801/2/84']]]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a0['introduction --cts']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "2601a007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 67]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[]\n",
    "b = [3,67]\n",
    "a = a+b\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "8016194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1,b1 = extract_info(folder=papers[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "31e97355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['author_main', 'authors', 'title', 'abstract', 'introduction --text', 'physical model and numerical setup --text', 'summary of previous results on the pebble isolation mass for circular planets --text', 'results of hydrodynamical simulations --text', 'a simple fitting formula for the pebble isolation mass for eccentric planets --text', 'discussion and conclusions --text', 'sections', 'introduction --cts', 'physical model and numerical setup --cts', 'summary of previous results on the pebble isolation mass for circular planets --cts', 'results of hydrodynamical simulations --cts', 'a simple fitting formula for the pebble isolation mass for eccentric planets --cts', 'discussion and conclusions --cts'])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "7ba0ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "faltted_a0 = {key:a0[key] for key in a0.keys() if key=='introduction --cts'}\n",
    "faltted_a1 = {key:a1[key] for key in a1.keys() if key=='introduction --cts'}\n",
    "data = [faltted_a0,faltted_a1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "b347cf2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['author_main'])"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faltted_a0.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "6ab1babc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "[Statement.TypeError] Collections containing collections can not be stored in properties.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_324801/463407828.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Paper\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/hoopoe/lib/python3.7/site-packages/py2neo/bulk/__init__.py\u001b[0m in \u001b[0;36mcreate_nodes\u001b[0;34m(tx, data, labels, keys)\u001b[0m\n\u001b[1;32m    115\u001b[0m         supplied as value lists)\n\u001b[1;32m    116\u001b[0m     \"\"\"\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munwind_create_nodes_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hoopoe/lib/python3.7/site-packages/py2neo/database.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, cypher, parameters, **kwparameters)\u001b[0m\n\u001b[1;32m    989\u001b[0m                 result = self._connector.auto_run(cypher, parameters,\n\u001b[1;32m    990\u001b[0m                                                   \u001b[0mgraph_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m                                                   readonly=self.readonly)\n\u001b[0m\u001b[1;32m    992\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mCursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhydrant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hoopoe/lib/python3.7/site-packages/py2neo/client/__init__.py\u001b[0m in \u001b[0;36mauto_run\u001b[0;34m(self, cypher, parameters, graph_name, readonly)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0mcx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_acquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcypher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreadonly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mConnectionUnavailable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConnectionBroken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hoopoe/lib/python3.7/site-packages/py2neo/client/http.py\u001b[0m in \u001b[0;36mauto_run\u001b[0;34m(self, cypher, parameters, graph_name, readonly)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHTTPTransactionRef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocommit_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcypher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHTTPResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mHTTPResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHTTPTransactionRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hoopoe/lib/python3.7/site-packages/py2neo/client/http.py\u001b[0m in \u001b[0;36maudit\u001b[0;34m(self, tx)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_broken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mfailure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m: [Statement.TypeError] Collections containing collections can not be stored in properties."
     ]
    }
   ],
   "source": [
    "create_nodes(graph.auto(), data, labels={\"Paper\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bba1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf55931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ceb4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paper = get_data(papers[2])\n",
    "paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0102d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"Sections\"][\"Introduction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b8277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9dfc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = sent_tokenize(t) \n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adc34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent = nltk.corpus.treebank.tagged_sents()[22]\n",
    "words = word_tokenize(t)\n",
    "words_tagged = nltk.pos_tag(words)\n",
    "\n",
    "#print(nltk.ne_chunk(words_tagged, binary=False))\n",
    "tree = nltk.ne_chunk(words_tagged, binary=False)\n",
    "print(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da89f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "#for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "\n",
    "#VAN = re.compile(words_tagged, re.VERBOSE)\n",
    "for doc in sent_tagged:\n",
    "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern = IN):\n",
    "        print(nltk.sem.rtuple(rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('ieer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6533c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3943b35",
   "metadata": {},
   "source": [
    "### Pipline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5608f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(\"pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff07fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = extract_text(data[1], \"pdftotext_cli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6d444",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_entities(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec68ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = sent_tokenize(t, language='english', preserve_line=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3830a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(t[10])\n",
    "tags = nltk.pos_tag(words)\n",
    "tree = nltk.ne_chunk(tags, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2830d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557dffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.ne_chunk(tt[10], binary=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef2097a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d1f7c8244089e20d2c78566a506b89fdbba8a6881d1733d9abedc360ce9f38b"
  },
  "kernelspec": {
   "display_name": "hoopoe",
   "language": "python",
   "name": "hoopoe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
