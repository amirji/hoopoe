{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd600bed",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09bd2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pdftotext\n",
    "from PyPDF2 import PdfFileReader\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ff539",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDict(dict):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "    def __getitem__(self,key):\n",
    "        return dict.__getitem__(self, key.lower())\n",
    "    def __setitem__(self,key,value):\n",
    "        return dict.__setitem__(self,key.lower(), value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde8ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(folder_name):\n",
    "    \"\"\"\n",
    "    This function assumes that the folder \"folder_name\" is stored inside the the notebooks folder.\n",
    "    Params:\n",
    "    folder_name: string\n",
    "    Returns: the paths to the data files in the given folder as a list.\n",
    "    \"\"\"\n",
    "    arr = os.listdir(folder_name)\n",
    "    return [os.path.join(os.getcwd(),folder_name, a) for a in arr]\n",
    "\n",
    "\n",
    "def pdftotext_wrapper(input_file, options=None, output_file=None):\n",
    "    \"\"\"\n",
    "    This function wraps the pdftotext command line tool.\n",
    "    Params:\n",
    "    input_file: string of path to the input pdf file.\n",
    "    output_file: string of path to the output text file.\n",
    "    options: string\n",
    "    Returns: the text as a string.\n",
    "    \"\"\"\n",
    "    if options is None:\n",
    "        options = \"\"\n",
    "\n",
    "    if output_file is None:\n",
    "        output_file = \"\"\n",
    "\n",
    "    check = os.popen(\"pdftotext \" + options + \" \" + input_file + \" \" + output_file).read()\n",
    "    if check == \"\":\n",
    "        return \"Success\"\n",
    "\n",
    "\n",
    "def extract_text(path, method):\n",
    "    \"\"\"\n",
    "    This function extracts the text from a pdf file.\n",
    "    Params:\n",
    "    path: string\n",
    "    method: string\n",
    "    Returns: the text as a string.\n",
    "    \"\"\"\n",
    "    if method == \"pdftotext_cli\":\n",
    "        file_name = path.replace(os.path.dirname(data[0])+\"/\", \"\").replace(\".pdf\", \"\")\n",
    "        output_dir = os.path.join(os.getcwd(), \"texts\")\n",
    "        output_file = os.path.join(output_dir, file_name + \".txt\")\n",
    "        pdftotext_wrapper(data[0], \"-raw\", output_file) \n",
    "        with open(output_file, 'r') as f:\n",
    "            #return f.read()\n",
    "            return f.read().replace(\"\\n\", \" \")\n",
    "            #return f.readlines()\n",
    "\n",
    "    if method == \"pdftotext_python\":\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pdftotext.PDF(f)\n",
    "\n",
    "    if method == \"pypdf2\":\n",
    "        text = []\n",
    "        with open(path, \"rb\") as f:\n",
    "            pdf = PdfFileReader(f)\n",
    "            text = [pdf.getPage(i).extractText() for i in range(pdf.numPages)]\n",
    "            return text\n",
    "\n",
    "def extract_entities(quote):\n",
    "    words = word_tokenize(quote)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    tree = nltk.ne_chunk(tags, binary=False)\n",
    "    return set(\n",
    "        \" \".join(i[0] for i in t)\n",
    "        for t in tree if hasattr(t, \"label\") and t.label() != \"NE\"\n",
    "    )\n",
    "\n",
    "def extract_info(folder, source=\"Tex\"):\n",
    "    \"\"\"\n",
    "    This function extracts the information from a paper using different methods and returns it as a dictionary with the following keys:\n",
    "    {\n",
    "        \"Author/Authors\": string,\n",
    "        \"Title\": string,\n",
    "        \"Year\": string,\n",
    "        \"Journal\": string,\n",
    "        \"Volume\": string,\n",
    "        \"Pages\": string,\n",
    "        \"Abstract\": string,\n",
    "        \"Sections\": list of strings,\n",
    "        \"References_Sections\": list of pairs of strings (refrence, section),\n",
    "        \"refrences\": list of strings,\n",
    "        \"Keywords\": string,\n",
    "        \"Language\": string,\n",
    "        \"Source\": string,\n",
    "        }\n",
    "    Params:\n",
    "    paper: string\n",
    "    source: string\n",
    "    Returns: a dictionary with the extracted information.\n",
    "\n",
    "    Reg Tips:\n",
    "    1. r\"\\\\author.*?\\\\\\\\\" -> mathc from \\authors command till the first \\\\ using lazy match in the Tex file.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    info = MyDict()\n",
    "\n",
    "    if source != \"Tex\":\n",
    "        return info\n",
    "    \n",
    "\n",
    "    contents = get_data(folder)\n",
    " \n",
    "    tex_files = [ct for ct in contents if os.path.splitext(ct)[1] == '.tex']\n",
    "    bib_files = [ct for ct in contents if os.path.splitext(ct)[1] == '.bib']\n",
    "\n",
    "\n",
    "    text = \"\"\n",
    "    for tex_file in tex_files:\n",
    "        with open(tex_file, \"r\") as f:\n",
    "            temp = f.read()\n",
    "            text+= \"\\n\"+temp\n",
    "    \n",
    "    refs = \"\"\n",
    "    for bib_file in bib_files:\n",
    "        with open(bib_file, \"r\") as f:\n",
    "            temp = f.read()\n",
    "            refs+= \"\\n\"+temp\n",
    "\n",
    "    # Remove all comments:\n",
    "    #text = re.sub(r\"\\%.*?\\n\", \"\", text,  re.DOTALL)\n",
    "\n",
    "\n",
    "\n",
    "    # Extracting the author(s)\n",
    "    #authors = re.findall(r\"\\\\author\\{(.*?)\\}\", text)\n",
    " \n",
    "    Author_main = re.findall(r\"\\\\author\\[(.*?)\\]\", text)\n",
    "    temp_authors = re.findall(r\"\\\\author.*?\\\\\\\\\", text, re.DOTALL)\n",
    "    temp_authors = re.findall(r\"\\].*?\\\\\\\\\", temp_authors[0], re.DOTALL)\n",
    "    temp_authors = re.findall(r\"(([A-Zéúßäüö]\\.?\\s?)*([A-Zéúßäüö][a-zéúßäüö]+\\.?\\s?)+([A-Zéúßäüö]\\.?\\s?[a-zéúßäüö]*)*)\", temp_authors[0], re.DOTALL)\n",
    "    Authors = [aut[0] for aut in temp_authors]\n",
    "\n",
    "\n",
    "    info[\"Author_main\"] = \"; \".join(Author_main)\n",
    "    info[\"Authors\"] = \"; \".join(Authors)\n",
    "\n",
    "    # Extracting the title\n",
    "    title_temp = re.findall(r\"\\\\title.*?]\", text, re.DOTALL)\n",
    "    title = re.findall(r\"\\[(.*?)]\", title_temp[0], re.DOTALL)\n",
    "    info[\"Title\"] = title[0]\n",
    "\n",
    "    # Extracting the Abstract\n",
    "    abstract_temp = re.findall(r\"\\\\begin{abstract}(.*?)\\\\end{abstract}\", text, re.DOTALL)\n",
    "    \n",
    "    # remove comments\n",
    "    abstract = re.sub(r\"\\%.*?\\n\", \"\", abstract_temp[0],  re.DOTALL)\n",
    "    info[\"Abstract\"] = abstract\n",
    "\n",
    "\n",
    "    # Find titles of sections\n",
    "    section_titles = re.findall(r\"\\\\section{(.*?)}\", text, re.DOTALL)\n",
    "\n",
    "    \n",
    "\n",
    "    # Extract text of each section:\n",
    "\n",
    "    sections_text = []\n",
    "    Sections = MyDict()\n",
    "    for s_t in section_titles:\n",
    "        section_grammer= r\"\\\\section{\" + s_t + \"}\" + r\"(.*?)\" + r\"\\\\section\"\n",
    "        temp =  re.findall(section_grammer, text, re.DOTALL)\n",
    "        sections_text.append(temp)\n",
    "        Sections[s_t] = MyDict({\"text\": temp[0]})\n",
    "    info[\"Sections\"] = Sections\n",
    "\n",
    "\n",
    "    # Extracting the citeations in each section:\n",
    "    for s_t in info[\"Sections\"]:\n",
    "        # Citation style: \\cite[][]{ref1, ref2, ref3}     \n",
    "        temp = re.findall(r\"\\\\citep.*?{(.*?)}\", info[\"Sections\"][s_t][\"text\"], re.DOTALL)\n",
    "        info[\"Sections\"][s_t][\"citations\"] = [t.split(\",\") for t in temp]\n",
    "\n",
    "        # Citation style: \\citet[][]{ref1, ref2, ref3} \n",
    "        temp = re.findall(r\"\\\\citet.*?{(.*?)}\", info[\"Sections\"][s_t][\"text\"], re.DOTALL)\n",
    "        info[\"Sections\"][s_t][\"citations\"].append([t.split(\",\") for t in temp])\n",
    "\n",
    "        # Citation style: \\citealp[][]{ref1, ref2, ref3} \n",
    "        temp = (re.findall(r\"\\\\citealp.*?{(.*?)}\", info[\"Sections\"][s_t][\"text\"], re.DOTALL))\n",
    "        info[\"Sections\"][s_t][\"citations\"].append([t.split(\",\") for t in temp])\n",
    "\n",
    "\n",
    "\n",
    "    citations_info = MyDict()\n",
    "    # Adding citations from bib files to the sections:\n",
    "    bib_cts = []\n",
    "    all_bib_cts = []\n",
    "    for s_t in info[\"Sections\"]:\n",
    "        all_cts = info[\"Sections\"][s_t][\"citations\"]\n",
    "        for cts in all_cts:\n",
    "            for ct in cts:\n",
    "                if len(ct)==1:\n",
    "                    tmp = re.findall(r\"(\\@\\w+.?(?=\"+ct[0]+r\")(.*?))\\@\", refs, re.DOTALL)\n",
    "                else:\n",
    "                    tmp = re.findall(r\"(\\@\\w+.?(?=\"+ct+r\")(.*?))\\@\", refs, re.DOTALL)\n",
    "              #  print(tmp)\n",
    "                bib_cts.append(tmp)\n",
    "            all_bib_cts.append(bib_cts)\n",
    "        citations_info[s_t] = all_bib_cts\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "    return info, citations_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec02afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = get_data(\"Texs\")\n",
    "a,b = extract_info(folder=papers[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0fd6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"Sections\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a6c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "b[\"Introduction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b922dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"Sections\"]['RESULTS OF HYDRODYNAMICAL SIMULATIONS'.lower()][\"citations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdebff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_df = pd.DataFrame(a[\"Sections\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bba1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf55931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ceb4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paper = get_data(papers[2])\n",
    "paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0102d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"Sections\"][\"Introduction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b8277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9dfc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = sent_tokenize(t) \n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adc34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent = nltk.corpus.treebank.tagged_sents()[22]\n",
    "words = word_tokenize(t)\n",
    "words_tagged = nltk.pos_tag(words)\n",
    "\n",
    "#print(nltk.ne_chunk(words_tagged, binary=False))\n",
    "tree = nltk.ne_chunk(words_tagged, binary=False)\n",
    "print(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da89f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "#for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "\n",
    "#VAN = re.compile(words_tagged, re.VERBOSE)\n",
    "for doc in sent_tagged:\n",
    "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern = IN):\n",
    "        print(nltk.sem.rtuple(rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('ieer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6533c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3943b35",
   "metadata": {},
   "source": [
    "### Pipline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5608f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(\"pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff07fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = extract_text(data[1], \"pdftotext_cli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6d444",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_entities(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec68ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = sent_tokenize(t, language='english', preserve_line=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3830a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(t[10])\n",
    "tags = nltk.pos_tag(words)\n",
    "tree = nltk.ne_chunk(tags, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2830d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557dffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.ne_chunk(tt[10], binary=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef2097a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d1f7c8244089e20d2c78566a506b89fdbba8a6881d1733d9abedc360ce9f38b"
  },
  "kernelspec": {
   "display_name": "hoopoe",
   "language": "python",
   "name": "hoopoe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
